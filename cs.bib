@Book{Vand_python_zh,
  author =       {VanderPlas, Jake},
  title =        {Python 数据科学手册},
  year =         2018,
  publisher =    {人民邮电出版社},
  note =         {陶俊杰陈小莉译},
}

@Book{Matth_python_2nd,
  author =       {Matthes, Eric},
  title =        {Python crash course: a hands-on, project-based introduction to
                  programming},
  year =         2019,
  publisher =    {No Starch Press},
  url =          {https://github.com/ehmatthes/pcc_2e},
  edition =      {2nd},
}

@Book{Matth_python_zh,
  author =       {Matthes, Eric},
  title =        {Python 编程：从入门到实践},
  year =         2016,
  publisher =    {人民邮电出版社},
  url =          {https://github.com/ehmatthes/pcc},
  note =         {袁国忠译},
  tags =         {DONE},
}

@Book{Matth_python_zh_2nd,
  author =       {Matthes, Eric},
  title =        {Python 编程（第二版）：从入门到实践},
  year =         2020,
  publisher =    {人民邮电出版社},
  url =          {https://book.douban.com/subject/35196328/},
  note =         {袁国忠译},
}

@Book{Fowl_concur_22,
  author =       {Fowler, Matthew},
  title =        {Python Concurrency with asyncio},
  year =         2022,
  publisher =    {Manning},
  url =          {https://book.douban.com/subject/35219949/},
}

@Book{Ramalho_python_zh,
  author =       {Ramalho, Luciano},
  title =        {流畅的 Python},
  year =         2017,
  publisher =    {人民邮电出版社},
  note =         {安道吴珂译},
}

@Book{Oner_esp32_21,
  author =       {Oner, Vedat Ozan},
  title =        {Developing IoT Projects with ESP32},
  year =         2021,
  publisher =    {Packt Publishing},
}

@Book{Kolban_eps32_18,
  author =       {Kolban, Neil},
  title =        {Kolban's book on ESP32},
  year =         2018,
  publisher =    {LearnPub},
  edition =      {2018-09-03},
}

@Manual{CC2591_datasheet,
  edition =      {Rev B},
  month =        {September},
  organization = {TI},
  title =        {CC2591 2.4-GHz RF Front End},
  year =         2014,
}

@Manual{CC2400_datasheet,
  edition =      {Rev A},
  month =        {March},
  organization = {TI},
  title =        {CC2400 2.4 GHz Low-Power RF Transceiver},
  year =         2006,
}

@Book{Ramalho_Fluent_2nd,
  author =       {Ramalho, Luciano},
  title =        {Fluent Python},
  year =         2022,
  publisher =    {O'Reilly},
  edition =      {2nd},
  month =        {April},
}

@Book{HWL_grl_2020,
  author =       {Hamilton, William L.},
  title =        {Graph representation learning},
  year =         2020,
  publisher =    {Morgan & Claypool publishers},
  edition =      {Pre-publication draft},
  series =       {Synthesis Lectures on Artificial Intelligence and Machine
                  Learning},
  volume =       14,
}

@Book{LLZ_gnn_19,
  author =       {刘忠雨 and 李彦霖 and 周洋},
  title =        {深入浅出图神经网络：GNN 原理解析},
  year =         2019,
  publisher =    {机械工业出版社},
}

@Book{Farrelly_Graph_Theory_Python_2024,
  author =       {Colleen M. Farrelly and Franck Kalala Mutombo},
  title =        {Modern Graph Theory Algorithms with Python：Harness the power
                  of graph algorithms and real-world network applications using
                  Python},
  year =         2024,
  publisher =    {Packt Publishing},
  url =          {https://book.douban.com/subject/36967601/},
}

@Misc{LLM_DSPy_intro_24,
  author =       {字节},
  howpublished = {知乎},
  month =        {June},
  title =        {LLM 应用框架解码之：DSPy},
  url =          {https://zhuanlan.zhihu.com/p/702228666},
  year =         2024,
}

@article{Pil_clifford_23,
  author =       {Pilanci, Mert},
  title =        {From Complexity To Clarity: Analytical Expressions of Deep
                  Neural Network Weights Via Clifford's Geometric Algebra and
                  Convexity},
  journal =      {CoRR},
  year =         2023,
  url =          {http://arxiv.org/abs/2309.16512v4},
  abstract =     {In this paper, we introduce a novel analysis of neural
                  networks based on geometric (Clifford) algebra and convex
                  optimization. We show that optimal weights of deep ReLU neural
                  networks are given by the wedge product of training samples
                  when trained with standard regularized loss. Furthermore, the
                  training problem reduces to convex optimization over wedge
                  product features, which encode the geometric structure of the
                  training dataset. This structure is given in terms of signed
                  volumes of triangles and parallelotopes generated by data
                  vectors. The convex problem finds a small subset of samples
                  via $\ell_1$ regularization to discover only relevant wedge
                  product features. Our analysis provides a novel perspective on
                  the inner workings of deep neural networks and sheds light on
                  the role of the hidden layers.},
  archivePrefix ={arXiv},
  eprint =       {2309.16512},
  primaryClass = {cs.LG},
}

@article{QBY_gump_24,
  author =       {Qiu, Haiquan and Bian, Yatao and Yao, Quanming},
  title =        {Graph Unitary Message Passing},
  journal =      {CoRR},
  year =         2024,
  url =          {http://arxiv.org/abs/2403.11199v1},
  abstract =     {Message passing mechanism contributes to the success of GNNs
                  in various applications, but also brings the oversquashing
                  problem. Recent works combat oversquashing by improving the
                  graph spectrums with rewiring techniques, disrupting the
                  structural bias in graphs, and having limited improvement on
                  oversquashing in terms of oversquashing measure. Motivated by
                  unitary RNN, we propose Graph Unitary Message Passing (GUMP)
                  to alleviate oversquashing in GNNs by applying unitary
                  adjacency matrix for message passing. To design GUMP, a
                  transformation is first proposed to make general graphs have
                  unitary adjacency matrix and keep its structural bias. Then,
                  unitary adjacency matrix is obtained with a unitary projection
                  algorithm, which is implemented by utilizing the intrinsic
                  structure of unitary adjacency matrix and allows GUMP to be
                  permutation-equivariant. Experimental results show the
                  effectiveness of GUMP in improving the performance on various
                  graph learning tasks.},
  archivePrefix ={arXiv},
  eprint =       {2403.11199},
  primaryClass = {cs.LG},
}

@article{HTF_FPI_24,
  author =       {Hirono, Yuji and Tanaka, Akinori and Fukushima, Kenji},
  title =        {Understanding Diffusion Models By Feynman's Path Integral},
  journal =      {CoRR},
  year =         2024,
  url =          {http://arxiv.org/abs/2403.11262v1},
  abstract =     {Score-based diffusion models have proven effective in image
                  generation and have gained widespread usage; however, the
                  underlying factors contributing to the performance disparity
                  between stochastic and deterministic (i.e., the probability
                  flow ODEs) sampling schemes remain unclear. We introduce a
                  novel formulation of diffusion models using Feynman's path
                  integral, which is a formulation originally developed for
                  quantum physics. We find this formulation providing
                  comprehensive descriptions of score-based generative models,
                  and demonstrate the derivation of backward stochastic
                  differential equations and loss functions.The formulation
                  accommodates an interpolating parameter connecting stochastic
                  and deterministic sampling schemes, and we identify this
                  parameter as a counterpart of Planck's constant in quantum
                  physics. This analogy enables us to apply the
                  Wentzel-Kramers-Brillouin (WKB) expansion, a well-established
                  technique in quantum physics, for evaluating the negative
                  log-likelihood to assess the performance disparity between
                  stochastic and deterministic sampling schemes.},
  archivePrefix ={arXiv},
  eprint =       {2403.11262},
  primaryClass = {cs.LG},
}

@Book{Ramalho_python_2nd_zh,
  author =       {Ramalho, Luciano},
  title =        {流畅的 Python （第二版）},
  year =         2023,
  publisher =    {人民邮电出版社},
  note =         {安道译},
}
